{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSimilarityLSH.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcD94LJhIRRNAB5bHyGh+B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lqcgithub/MiningofMassiveDatasets/blob/main/TextSimilarityLSH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "echGsTg6uVB4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyRwS6eWvpPb",
        "outputId": "edf8902d-bd86-4cfd-863c-251dcc50dee5"
      },
      "source": [
        "pip install beautifulsoup4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE3j9_7tCewn",
        "outputId": "c5c79745-551d-4f2d-871d-cb368f5a0963"
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 61kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 18.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=85619e857427518f7dd0d84e5462bf853c429a086f2baf3f338cb83c633c3c93\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhiSIN_ExYmo"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-88_CK1hCiFv"
      },
      "source": [
        "import pyspark\r\n",
        "from pyspark import SparkConf, SparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBiFOj80C2RM"
      },
      "source": [
        "conf = SparkConf().setMaster(\"local\").setAppName(\"Market Basket with Apriori\")\r\n",
        "sc = SparkContext.getOrCreate(conf=conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6wegdagv2Fn"
      },
      "source": [
        "baseUrl = \"https://tuoitre.vn\"\r\n",
        "url = \"https://tuoitre.vn/tin-moi-nhat.htm\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIIh3kdQzeDR"
      },
      "source": [
        "response = requests.get(url)\r\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\r\n",
        "\r\n",
        "titles = soup.findAll('h3', class_='title-news')\r\n",
        "links = [link.find('a').attrs[\"href\"] for link in titles]\r\n",
        "data = []\r\n",
        "\r\n",
        "for link in links:\r\n",
        "  news = requests.get(baseUrl + link)\r\n",
        "  soup = BeautifulSoup(news.content, \"html.parser\")\r\n",
        "  if (soup.find(\"h1\", class_=\"article-title\")is not None):\r\n",
        "    title = soup.find(\"h1\", class_=\"article-title\").text\r\n",
        "    abstract = soup.find(\"h2\", class_=\"sapo\").text\r\n",
        "    body = soup.find(\"div\", id=\"main-detail-body\")\r\n",
        "    contents = body.findAll(\"p\", recursive=False)\r\n",
        "    contents = [content.text for content in contents]\r\n",
        "    content = ('').join(contents)\r\n",
        "    data.append({\r\n",
        "        \"title\": title,\r\n",
        "        \"abstract\": abstract,\r\n",
        "        \"content\": content,\r\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx6-nflbBppJ",
        "outputId": "95a318d1-a3dd-40cb-f0d8-690799901be2"
      },
      "source": [
        "print(data[0][\"content\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trận này, Leicester áp đảo ngay sau tiếng còi khai cuộc. Đây cũng là điều dễ hiểu, bởi đối thủ của họ - Sheffield United là đội bóng đang xếp cuối bảng và gần như cầm chắc vé xuống hạng khi kém nhóm an toàn tới 14 điểm.Phút 12, cơ hội mở tỉ số đến với Leicester sau pha dàn xếp tấn công bên cánh trái. Nhưng may mắn đã không đứng về phía đội chủ nhà khi pha đánh đầu về góc xa của Ayoze Perez lại đưa bóng trúng cột dọc khung thành.  Những phút sau đó, Leicester tiếp tục tấn công và tạo ra thêm nhiều tình huống nguy hiểm nữa. Dù áp đảo hoàn toàn, nhưng phải đến cuối hiệp 1 các học trò HLV Brendan Rodgers mới đưa được bóng vào lưới khung thành thủ môn Aaron Ramsdale.  Phút 39, từ pha lên bóng bên cánh trái, Jamie Vardy bứt tốc xâm nhập vòng cấm đón đường chọc khe của đồng đội rồi thực hiện đường căng ngang vào giữa. Trong tư thế không người kèm, Kelechi Iheanacho dễ dàng ập vào đệm bóng tung lưới thủ môn Aaron Ramsdale từ cự li gần, mở tỉ số cho Leicester.  Bị thủng lưới, Sheffield United đẩy cao đội hình tấn công trong hiệp 2 và cơ hội tiếp tục đến với Leicester khi hàng phòng ngự đội khách để lộ ra nhiều khoảng trống.  Phút 64, sau vài cơ hội bị bỏ lỡ, đội chủ nhà cũng đã có bàn thắng nhân đôi cách biệt nhờ công tiền vệ Ayoze Perez với pha dứt điểm hiểm hóc ngay mép vòng cấm sau tình huống phản công bên cánh phải.  Sau bàn thua này, các cầu thủ Sheffield United càng thi đấu thiếu tập trung, đặc biệt là trong phòng ngự.  Hệ quả, chỉ trong 11 phút từ 69 đến 80, lưới của thủ môn Aaron Ramsdale đã phải rung lên thêm 3 lần nữa sau những cú dứt điểm trong tư thế vô cùng thoải mái của Kelechi Iheanacho (phút 69 và 78) và Jamie Vardy (phút 80), giúp Leicester khép lại trận đấu với chiến thắng cách biệt 5-0.  Thắng trận này, Leicester đã tạm vượt qua Manchester United (chưa đá vòng 28) để vươn lên vị trí thứ hai trên bảng xếp hạng với 56 điểm, hơn Man Utd 2 điểm và kém đội đầu bảng Man City 15 điểm.  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owmwcN5Ut9QF"
      },
      "source": [
        "# def crawNewsData(baseUrl, url):\r\n",
        "#     response = requests.get(url)\r\n",
        "#     soup = BeautifulSoup(response.content, \"html.parser\")\r\n",
        "\r\n",
        "#     titles = soup.findAll('h3', class_='title-news')\r\n",
        "#     links = [link.find('a').attrs[\"href\"] for link in titles]\r\n",
        "#     data = []\r\n",
        "#     for link in links:\r\n",
        "#         news = requests.get(baseUrl + link)\r\n",
        "#         soup = BeautifulSoup(news.content, \"html.parser\")\r\n",
        "#         title = soup.find(\"h1\", class_=\"article-title\").text\r\n",
        "#         abstract = soup.find(\"h2\", class_=\"sapo\").text\r\n",
        "#         body = soup.find(\"div\", id=\"main-detail-body\")\r\n",
        "#         content = \"\"\r\n",
        "#         try:\r\n",
        "#             contents = body.findAll(\"p\", recursive=False)\r\n",
        "#             contents = [content.text for content in contents]\r\n",
        "#             content = ('').join(contents)\r\n",
        "#         except:\r\n",
        "#             content = \"\"\r\n",
        "#         data.append({\r\n",
        "#             \"title\": title,\r\n",
        "#             \"abstract\": abstract,\r\n",
        "#             \"content\": content,\r\n",
        "#         })\r\n",
        "#     return data"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}